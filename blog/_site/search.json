[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Eros Rojas",
    "section": "",
    "text": "R\n\n\nDSCI-100\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2022\n\n\nEros Rojas\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2022\n\n\nEros Rojas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a 3rd year undergraduate student studying Mathematics and Data Science at UBC, where I am also a TA for DSCI-100."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "My Old DSCI-100 Project",
    "section": "",
    "text": "TLDR: Do not assume anything of your data. Often times you will be wrong.\n  \nBack when I took DSCI-100, I was very new to the data science and statistical modeling world. I was vastly unaware of the absurd number of specialized algorithms created for basically any and all types of problems.\nThis was one of my first full data science reports, and as a result of my previously limited knowledge, there are numerous aspects of the report that are misleading, or flat out misrepresentative of reality. I wish to make this post as a point of progression to see how much I have learned in the past two years, and to be able to recognize what types of mistakes data science beginners generally make. Think of it like a video game ‘remaster’, except the difference being that nobody asked for it.\nBelow you will find my DSCI-100 final report, but with the added bonus of italicized annotations alongside all of the questionable statements and pieces of code that were written."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "First Post",
    "section": "",
    "text": "I am creating this blog as a way to put myself and all of my projects out there, in hopes that it may help somebody in some way. \nFor now, here is a photo of Nice, France that I found on Google."
  },
  {
    "objectID": "posts/post-with-code/index.html#predicting-covid-19-transmission-risk",
    "href": "posts/post-with-code/index.html#predicting-covid-19-transmission-risk",
    "title": "My Old DSCI-100 Project",
    "section": "Predicting COVID-19 Transmission Risk",
    "text": "Predicting COVID-19 Transmission Risk\n\nIntroduction:\nWe plan to help out Billy, who has recently lost his job due to COVID-19 and is trying to find a job in another country. He prefers to move to a country that will be safer, with fewer new COVID cases.\nWe aim to help Billy answer the question: “How safe is it to move to country X?” by predicting the number of new_cases_per_million through KNN-Regression. A lower number of cases indicates a safer country with lower transmission risk. We will use a categorical variable “risk level”, that we created ourselves, to more easily interpret the numerical output of our prediction.\nWe will use worldwide data collected over the past few months of the pandemic (Ritchie 2020). This dataset contains many variables, but we will use population density, stringency index (a composite measure of government COVID-19 response), GDP per capita (economic output per person), hospital beds per thousand, and life expectancy as predictors. We narrowed the data down to consider only current data (from September 1st 2020 onwards), to accurately model the current coronavirus situation. We will predict the number of new cases per million, using our designated predictor variables, to inform Billy’s decision.\n\n\nShow the code\n# installing 3rd party packages required for running this file\ninstall.packages(\"lubridate\", repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"GGally\", repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"reshape2\", repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"shiny\", repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"repr\", repos = \"http://cran.us.r-project.org\")\n\n# loading packages which run on DSCI server\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(repr)\n\n# loading installed packages\nlibrary(lubridate)\nlibrary(GGally)\nlibrary(reshape2)\nlibrary(shiny)\n\n\nThe data has been obtained from the ‘Our World in Data,’ Coronavirus Source Dataset, which is collected in partnership with the University of Oxford and the Oxford Martin School, updated daily.\n\n\nShow the code\n# utilizng the github commit hash code in order to maintain the same running code throughout the lifespan of this project\nurl <- \"https://raw.githubusercontent.com/owid/covid-19-data/fb73a3759b6691dc9a6f880353a37e70cd7ceb92/public/data/owid-covid-data.csv\"\ndata <- read_csv(url, show_col_types = FALSE)\n\n# adding table title\nstrong(\"Table 1: Raw data from 'Coronavirus Source Data'\")\nhead(data)\n\n\nTable 1: Raw data from 'Coronavirus Source Data'\n\n## # A tibble: 6 x 50\n##   iso_code continent location  date       total_cases new_cases new_cases_smoot~\n##   <chr>    <chr>     <chr>     <date>           <dbl>     <dbl>            <dbl>\n## 1 AFG      Asia      Afghanis~ 2019-12-31          NA         0               NA\n## 2 AFG      Asia      Afghanis~ 2020-01-01          NA         0               NA\n## 3 AFG      Asia      Afghanis~ 2020-01-02          NA         0               NA\n## 4 AFG      Asia      Afghanis~ 2020-01-03          NA         0               NA\n## 5 AFG      Asia      Afghanis~ 2020-01-04          NA         0               NA\n## 6 AFG      Asia      Afghanis~ 2020-01-05          NA         0               NA\n## # ... with 43 more variables: total_deaths <dbl>, new_deaths <dbl>,\n## #   new_deaths_smoothed <dbl>, total_cases_per_million <dbl>,\n## #   new_cases_per_million <dbl>, new_cases_smoothed_per_million <dbl>,\n## #   total_deaths_per_million <dbl>, new_deaths_per_million <dbl>,\n## #   new_deaths_smoothed_per_million <dbl>, reproduction_rate <dbl>,\n## #   icu_patients <dbl>, icu_patients_per_million <dbl>, hosp_patients <dbl>,\n## #   hosp_patients_per_million <dbl>, weekly_icu_admissions <dbl>, ...\n\nWe begin by selecting the columns which will be useful for exploratory analysis. We will be visualizing the ‘new_cases_smoothed_per_million’ to represent a smoothed average (prettier/smoother plots!). However, for our predictive model, we will be using ‘new_cases_per_million’ for a more specific estimate of new cases for one given day.\nWe restricted the observations to those taken past September 1st 2020 – to ensure recent data and prevent overplotting – and before November 16th 2020, since Billy was booking his flight on November 17th. Therefore, the model will be unable to ‘cheat’ by seeing the data that Billy used to make his prediction/decision.\ntesting testing testing\n\n\nShow the code\n# this is our exploratory data table, depicting our predictors of interest\nnewdata <- data %>%\n  select(location, date, total_cases, new_cases, new_cases_per_million ,new_cases_smoothed_per_million, stringency_index, population_density, gdp_per_capita, hospital_beds_per_thousand, life_expectancy) %>% \n  filter(date >= as.Date(\"2020-09-01\"), date <= as.Date(\"2020-11-16\")) %>% #filter date after september to avoid overplotting\n  filter(new_cases_smoothed_per_million >= 0 & new_cases_smoothed_per_million < 1000) %>% #stating our upper and lower boundries\n  drop_na() # we are dropping our NA values due to the fact that we can not preform KNN regression on values which do not exist \n\n# adding table title\nstrong(\"Table 2: Preliminary data filtering\")\nhead(newdata)\n\n\nTable 2: Preliminary data filtering\n\n## # A tibble: 6 x 11\n##   location    date       total_cases new_cases new_cases_per_m~ new_cases_smoot~\n##   <chr>       <date>           <dbl>     <dbl>            <dbl>            <dbl>\n## 1 Afghanistan 2020-09-01       38196        34            0.873            0.462\n## 2 Afghanistan 2020-09-02       38205         9            0.231            0.492\n## 3 Afghanistan 2020-09-03       38243        38            0.976            0.429\n## 4 Afghanistan 2020-09-04       38288        45            1.16             0.583\n## 5 Afghanistan 2020-09-05       38304        16            0.411            0.602\n## 6 Afghanistan 2020-09-06       38324        20            0.514            0.664\n## # ... with 5 more variables: stringency_index <dbl>, population_density <dbl>,\n## #   gdp_per_capita <dbl>, hospital_beds_per_thousand <dbl>,\n## #   life_expectancy <dbl>"
  }
]