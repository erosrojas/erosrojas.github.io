[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Eros Rojas",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2023\n\n\nEros Rojas\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nDSCI-100\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2022\n\n\nEros Rojas\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2022\n\n\nEros Rojas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a 3rd year undergraduate student studying Mathematics and Data Science at UBC, where I am also a TA for DSCI-100."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "My Old DSCI-100 Project",
    "section": "",
    "text": "TLDR: Do not assume anything of your data. Often times you will be wrong.\n  \nBack when I took DSCI-100, I was very new to the data science and statistical modeling world. I was vastly unaware of the absurd number of specialized algorithms created for basically any and all types of problems.\nThis was one of my first full data science reports, and as a result of my previously limited knowledge, there are numerous aspects of the report that are misleading, or flat out misrepresentative of reality. I wish to make this post as a point of progression to see how much I have learned in the past two years, and to be able to recognize what types of mistakes data science beginners generally make. Think of it like a video game ‘remaster’, except the difference being that nobody asked for it.\nBelow you will find my DSCI-100 final report, but with the added bonus of italicized annotations alongside all of the questionable statements and pieces of code that were written."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "First Post",
    "section": "",
    "text": "I am creating this blog as a way to put myself and all of my projects out there, in hopes that it may help somebody in some way. \nFor now, here is a photo of Nice, France that I found on Google."
  },
  {
    "objectID": "posts/post-with-code/index.html#predicting-covid-19-transmission-risk",
    "href": "posts/post-with-code/index.html#predicting-covid-19-transmission-risk",
    "title": "My Old DSCI-100 Project",
    "section": "Predicting COVID-19 Transmission Risk",
    "text": "Predicting COVID-19 Transmission Risk\n\nIntroduction:\nWe plan to help out Billy, who has recently lost his job due to COVID-19 and is trying to find a job in another country. He prefers to move to a country that will be safer, with fewer new COVID cases.\nWe aim to help Billy answer the question: “How safe is it to move to country X?” by predicting the number of new_cases_per_million through KNN-Regression. A lower number of cases indicates a safer country with lower transmission risk. We will use a categorical variable “risk level”, that we created ourselves, to more easily interpret the numerical output of our prediction.\nWe will use worldwide data collected over the past few months of the pandemic (Ritchie 2020). This dataset contains many variables, but we will use population density, stringency index (a composite measure of government COVID-19 response), GDP per capita (economic output per person), hospital beds per thousand, and life expectancy as predictors. We narrowed the data down to consider only current data (from September 1st 2020 onwards), to accurately model the current coronavirus situation. We will predict the number of new cases per million, using our designated predictor variables, to inform Billy’s decision.\n\n\nShow the code\n# installing 3rd party packages required for running this file\ninstall.packages(\"lubridate\", repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"GGally\", repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"reshape2\", repos = \"http://cran.us.r-project.org\")\n#install.packages(\"shiny\", repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"repr\", repos = \"http://cran.us.r-project.org\")\n\n# loading installed packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(repr)\nlibrary(lubridate)\nlibrary(GGally)\nlibrary(reshape2)\n#library(shiny)\n\n\nThe data has been obtained from the ‘Our World in Data,’ Coronavirus Source Dataset, which is collected in partnership with the University of Oxford and the Oxford Martin School, updated daily.\n\n\nShow the code\n# utilizng the github commit hash code in order to maintain the same running code throughout the lifespan of this project\nurl <- \"https://raw.githubusercontent.com/owid/covid-19-data/fb73a3759b6691dc9a6f880353a37e70cd7ceb92/public/data/owid-covid-data.csv\"\ndata <- read_csv(url, show_col_types = FALSE)\n\n# adding table title\n#strong(\"Table 1: Raw data from 'Coronavirus Source Data'\")\nhead(data)\n\n\n# A tibble: 6 × 50\n  iso_code continent location date       total…¹ new_c…² new_c…³ total…⁴ new_d…⁵\n  <chr>    <chr>     <chr>    <date>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 AFG      Asia      Afghani… 2019-12-31      NA       0      NA      NA       0\n2 AFG      Asia      Afghani… 2020-01-01      NA       0      NA      NA       0\n3 AFG      Asia      Afghani… 2020-01-02      NA       0      NA      NA       0\n4 AFG      Asia      Afghani… 2020-01-03      NA       0      NA      NA       0\n5 AFG      Asia      Afghani… 2020-01-04      NA       0      NA      NA       0\n6 AFG      Asia      Afghani… 2020-01-05      NA       0      NA      NA       0\n# … with 41 more variables: new_deaths_smoothed <dbl>,\n#   total_cases_per_million <dbl>, new_cases_per_million <dbl>,\n#   new_cases_smoothed_per_million <dbl>, total_deaths_per_million <dbl>,\n#   new_deaths_per_million <dbl>, new_deaths_smoothed_per_million <dbl>,\n#   reproduction_rate <dbl>, icu_patients <dbl>,\n#   icu_patients_per_million <dbl>, hosp_patients <dbl>,\n#   hosp_patients_per_million <dbl>, weekly_icu_admissions <dbl>, …\n\n\nWe begin by selecting the columns which will be useful for exploratory analysis. We will be visualizing the ‘new_cases_smoothed_per_million’ to represent a smoothed average (prettier/smoother plots!). However, for our predictive model, we will be using ‘new_cases_per_million’ for a more specific estimate of new cases for one given day.\nWe restricted the observations to those taken past September 1st 2020 – to ensure recent data and prevent overplotting – and before November 16th 2020, since Billy was booking his flight on November 17th. Therefore, the model will be unable to ‘cheat’ by seeing the data that Billy used to make his prediction/decision.\nincomplete, will be back another day to finish\n\n\nShow the code\n# this is our exploratory data table, depicting our predictors of interest\nnewdata <- data %>%\n  select(location, date, total_cases, new_cases, new_cases_per_million ,new_cases_smoothed_per_million, stringency_index, population_density, gdp_per_capita, hospital_beds_per_thousand, life_expectancy) %>% \n  filter(date >= as.Date(\"2020-09-01\"), date <= as.Date(\"2020-11-16\")) %>% #filter date after september to avoid overplotting\n  filter(new_cases_smoothed_per_million >= 0 & new_cases_smoothed_per_million < 1000) %>% #stating our upper and lower boundries\n  drop_na() # we are dropping our NA values due to the fact that we can not preform KNN regression on values which do not exist \n\n# adding table title\n#strong(\"Table 2: Preliminary data filtering\")\nhead(newdata)\n\n\n# A tibble: 6 × 11\n  location    date       total…¹ new_c…² new_c…³ new_c…⁴ strin…⁵ popul…⁶ gdp_p…⁷\n  <chr>       <date>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 Afghanistan 2020-09-01   38196      34   0.873   0.462    21.3    54.4   1804.\n2 Afghanistan 2020-09-02   38205       9   0.231   0.492    21.3    54.4   1804.\n3 Afghanistan 2020-09-03   38243      38   0.976   0.429    21.3    54.4   1804.\n4 Afghanistan 2020-09-04   38288      45   1.16    0.583    21.3    54.4   1804.\n5 Afghanistan 2020-09-05   38304      16   0.411   0.602    21.3    54.4   1804.\n6 Afghanistan 2020-09-06   38324      20   0.514   0.664    21.3    54.4   1804.\n# … with 2 more variables: hospital_beds_per_thousand <dbl>,\n#   life_expectancy <dbl>, and abbreviated variable names ¹​total_cases,\n#   ²​new_cases, ³​new_cases_per_million, ⁴​new_cases_smoothed_per_million,\n#   ⁵​stringency_index, ⁶​population_density, ⁷​gdp_per_capita"
  },
  {
    "objectID": "posts/multicollinearity/index.html",
    "href": "posts/multicollinearity/index.html",
    "title": "Multicollinearity: An Intuitive Example",
    "section": "",
    "text": "Visualized in 3 dimensions, with the help of plotly.\n   \n\n\nAs defined by Wikipedia, “multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.” Essentially, this means that an input variable (or multiple variables) is highly correlated with another input variable, to the extent that their relationship is virtually linearly dependent. This can cause numerous issues with respect to the regression analysis, most notably causing:\n\n\nInflated standard error\n\n\nUninterpretable + Unstable coefficients\n\n\nAll of these issues will be explained in the following 3 sections.  \n\n\nThe standard error of the coefficients of a linear model can be calculated as follows:  Lets assume a linear model has the following loss function\n\\[f(w) = ||Xw + \\epsilon - \\hat{y}||_2\\]\nwhere \\(\\epsilon\\) represents the error terms which follow a normal distribution, such that \\(\\epsilon \\sim N(0, \\sigma^2)\\). Since the given loss function above is fully differentiable and convex, we can find a closed-form solution for its coefficients as\n\\[w = (X^TX)^{-1}X^T\\hat{y}\\]\nNow that the coefficients are isolated, finding the variance should be quite straight forward. \\[\\begin{align}\n\\mathrm{Var}(w) &= \\mathrm{Var}\\left((X^TX)^{-1}X^T \\hat{y} \\right) \\\\\n&= (X^TX)^{−1}X^T \\cdot \\mathrm{Var}(\\hat{y}) \\cdot X(X^TX)^{−1} \\\\\n&= \\sigma^2 (X^T X)^{-1}\n\\end{align}\\] Note: \\(\\mathrm{Var}(\\hat{y}) = \\mathrm{Var}(\\epsilon)\\) due to the fact that \\(\\hat{y} = Xw + \\epsilon\\), and \\(Xw\\) has a variance of 0 (since \\(Xw\\) is not a random variable).  It is clear that \\(X^T X\\) must be a square matrix, therefore, it can be diagonalized such that\n\\[X^T X = PDP^{-1}\\]\nwhere \\(P\\) is an invertible matrix containing linearly independent eigenvectors of \\(X^T X\\), and \\(D\\) is a diagonal matrix containing the eigenvalues of \\(X^T X\\) (you can read more about matrix diagonalization here). Most notably, the product of the entries of \\(D\\) (the eigenvalues) are equal to the determinant of \\(D\\). When multicollinearity is present, \\(X^T X\\) becomes ‘nearly singular’ (a matrix which has a determinant very close to \\(0\\), and who’s columns are nearly linearly dependent. For reference, a non-singular matrix is linearly independent, and a singular matrix is linearly dependent and has a determinant of \\(0\\)). In other words, this means that one or more of the eigenvalues of \\(D\\) must be very close to \\(0\\), or in fact \\(0\\). Given the inverse of the diagonalized matrix above;\n\\[(X^T X)^{-1} = PD^{-1}P^{-1}\\]\nthis means that with multicollinearity, \\(D^{-1}\\) must have very large values. This is due to the fact that when the inverse of a matrix is taken, its determinant is essentially divided out from each element of the resulting matrix (this is an oversimplification, look into adjugate matrices if you are interested). This causes \\((X^T X)^{-1}\\) to subsequently also have very large values, which finally causes \\(\\mathrm{Var}(w)\\) to be very large. This shows why multicollinearity severely inflates the standard error of the coefficients.  \n\n\n\n\n\n\n\nIt might be a little difficult to digest exactly why the coefficients become highly unstable from the above explanation. To help give a visual aspect to the problem of collinearity, the following simulation was created to replicate the effects, and issues that arise with collinear features.\n\n\n\n\n\n          x1        x2         y\nx1 1.0000000 0.9999936 0.9699297\nx2 0.9999936 1.0000000 0.9699606\ny  0.9699297 0.9699606 1.0000000"
  },
  {
    "objectID": "posts/multicollinearity/index.html#understanding-what-collinearity-is",
    "href": "posts/multicollinearity/index.html#understanding-what-collinearity-is",
    "title": "Multicollinearity: An Intuitive Example",
    "section": "Understanding what collinearity is:",
    "text": "Understanding what collinearity is:\n\nUnderstanding what collinearity is:"
  },
  {
    "objectID": "posts/multicollinearity/index.html#an-intuitive-visualization-of-collinearity",
    "href": "posts/multicollinearity/index.html#an-intuitive-visualization-of-collinearity",
    "title": "Multicollinearity: An Intuitive Example",
    "section": "An intuitive visualization of collinearity",
    "text": "An intuitive visualization of collinearity\nVisualized in 3 dimensions, with the help of plotly.\n   \n\nUnderstanding what collinearity is:\nAs defined by Wikipedia, “multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.” Essentially, this means that an input variable (or multiple variables) is highly correlated with another input variable, to the extent that their relationship is virtually linearly dependent. This can cause numerous issues with respect to the regression analysis, most notably causing:\n\n\nInflated standard error\n\n\nUninterpretable + Unstable coefficients\n\n\nAll of these issues will be explained in the following 3 sections.  \n\nInflated standard error:\nThe standard error of the coefficients of a linear model can be calculated as follows:  Lets assume a linear model has the following loss function\n\\[f(w) = ||Xw + \\epsilon - \\hat{y}||_2\\]\nwhere \\(\\epsilon\\) represents the error terms which follow a normal distribution, such that \\(\\epsilon \\sim N(0, \\sigma^2)\\). Since the given loss function above is fully differentiable and convex, we can find a closed-form solution for its coefficients as\n\\[w = (X^TX)^{-1}X^T\\hat{y}\\]\nNow that the coefficients are isolated, finding the variance should be quite straight forward. \\[\\begin{align}\n\\mathrm{Var}(w) &= \\mathrm{Var}\\left((X^TX)^{-1}X^T \\hat{y} \\right) \\\\\n&= (X^TX)^{−1}X^T \\cdot \\mathrm{Var}(\\hat{y}) \\cdot X(X^TX)^{−1} \\\\\n&= \\sigma^2 (X^T X)^{-1}\n\\end{align}\\] Note: \\(\\mathrm{Var}(\\hat{y}) = \\mathrm{Var}(\\epsilon)\\) due to the fact that \\(\\hat{y} = Xw + \\epsilon\\), and \\(Xw\\) has a variance of 0 (since \\(Xw\\) is not a random variable).  It is clear that \\(X^T X\\) must be a square matrix, therefore, it can be diagonalized such that\n\\[X^T X = PDP^{-1}\\]\nwhere \\(P\\) is an invertible matrix containing linearly independent eigenvectors of \\(X^T X\\), and \\(D\\) is a diagonal matrix containing the eigenvalues of \\(X^T X\\) (you can read more about matrix diagonalization here). Most notably, the product of the entries of \\(D\\) (the eigenvalues) are equal to the determinant of \\(D\\). When multicollinearity is present, \\(X^T X\\) becomes ‘nearly singular’ (a matrix which has a determinant very close to \\(0\\), and who’s columns are nearly linearly dependent. For reference, a non-singular matrix is linearly independent, and a singular matrix is linearly dependent and has a determinant of \\(0\\)). In other words, this means that one or more of the eigenvalues of \\(D\\) must be very close to \\(0\\), or in fact \\(0\\). Given the inverse of the diagonalized matrix above;\n\\[(X^T X)^{-1} = PD^{-1}P^{-1}\\]\nthis means that with multicollinearity, \\(D^{-1}\\) must have very large values. This is due to the fact that when the inverse of a matrix is taken, its determinant is essentially divided out from each element of the resulting matrix (this is an oversimplification, look into adjugate matrices if you are interested). This causes \\((X^T X)^{-1}\\) to subsequently also have very large values, which finally causes \\(\\mathrm{Var}(w)\\) to be very large. This shows why multicollinearity severely inflates the standard error of the coefficients.  \n\n\nUninterpretable + Unstable coefficients:\n\n\n\nIntuition behind the instability:\nIt might be a little difficult to digest exactly why the coefficients become highly unstable from the above explanation. To help give a visual aspect to the problem of collinearity, the following simulation was created to replicate the effects, and issues that arise with collinear features.\n\n\n\n\n\n          x1        x2         y\nx1 1.0000000 0.9999936 0.9699297\nx2 0.9999936 1.0000000 0.9699606\ny  0.9699297 0.9699606 1.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource Code\n---\ntitle: \"Multicollinearity: An Intuitive Example\"\nauthor: \"Eros Rojas\"\ndate: \"2023-01-08\"\ncategories: [R, Statistics]\nimage: \"colinear.jpg\"\nformat:\n  html:\n    code-fold: show\n    code-summary: \"Show the code\"\n    code-tools: true\n    code-overflow: scroll\n    page-layout: custom\nserver: shiny\n---\n\n\n\n## **An intuitive visualization of collinearity**\nVisualized in 3 dimensions, with the help of plotly.\n\n<br>\n![](3d.jpg)\n<br>\n<br>\n\n### **Understanding what collinearity is:**\n\nAs defined by <a href=\"https://en.wikipedia.org/wiki/Multicollinearity\" target=\"_blank\">Wikipedia</a>, \"*multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.*\" Essentially, this means that an input variable (or multiple variables) is highly correlated with another input variable, to the extent that their relationship is virtually linearly dependent. This can cause numerous issues with respect to the regression analysis, most notably causing: \n\n<ul>\n    <li>Inflated standard error</li>\n    <li>Uninterpretable + Unstable coefficients</li>\n</ul>\n\nAll of these issues will be explained in the following 3 sections.\n<br>\n<br>\n\n#### **Inflated standard error:**\n\nThe standard error of the coefficients of a linear model can be calculated as follows: \n<br><br>\nLets assume a linear model has the following loss function\n$$f(w) = ||Xw + \\epsilon - \\hat{y}||_2$$\nwhere $\\epsilon$ represents the error terms which follow a normal distribution, such that $\\epsilon \\sim N(0, \\sigma^2)$. Since the given loss function above is fully differentiable and convex, we can find a closed-form solution for its coefficients as\n$$w = (X^TX)^{-1}X^T\\hat{y}$$\nNow that the coefficients are isolated, finding the variance should be quite straight forward.\n\\begin{align}\n\\mathrm{Var}(w) &= \\mathrm{Var}\\left((X^TX)^{-1}X^T \\hat{y} \\right) \\\\\n&= (X^TX)^{−1}X^T \\cdot \\mathrm{Var}(\\hat{y}) \\cdot X(X^TX)^{−1} \\\\\n&= \\sigma^2 (X^T X)^{-1}\n\\end{align}\n*Note: $\\mathrm{Var}(\\hat{y}) = \\mathrm{Var}(\\epsilon)$ due to the fact that $\\hat{y} = Xw + \\epsilon$, and $Xw$ has a variance of 0 (since $Xw$ is not a random variable)*.\n<br><br>\nIt is clear that $X^T X$ must be a square matrix, therefore, it can be diagonalized such that \n$$X^T X = PDP^{-1}$$\nwhere $P$ is an invertible matrix containing linearly independent eigenvectors of $X^T X$, and $D$ is a diagonal matrix containing the eigenvalues of $X^T X$ (you can read more about matrix diagonalization <a href=\"https://www.statlect.com/matrix-algebra/matrix-diagonalization\" target=\"_blank\">here</a>). Most notably, the product of the entries of $D$ (the eigenvalues) are equal to the determinant of $D$. When multicollinearity is present, $X^T X$ becomes 'nearly singular' (a matrix which has a determinant very close to $0$, and who's columns are nearly linearly dependent. For reference, a non-singular matrix is linearly independent, and a singular matrix is linearly dependent and has a determinant of $0$). In other words, this means that one or more of the eigenvalues of $D$ must be very close to $0$, or in fact $0$. Given the inverse of the diagonalized matrix above;\n$$(X^T X)^{-1} = PD^{-1}P^{-1}$$\nthis means that with multicollinearity, $D^{-1}$ must have very large values. This is due to the fact that when the inverse of a matrix is taken, its determinant is essentially divided out from each element of the resulting matrix (this is an oversimplification, look into <a href=\"https://en.wikipedia.org/wiki/Adjugate_matrix\" target=\"_blank\">adjugate matrices</a> if you are interested). This causes $(X^T X)^{-1}$ to subsequently also have very large values, which finally causes $\\mathrm{Var}(w)$ to be very large. This shows why multicollinearity severely inflates the standard error of the coefficients.\n<br>\n<br>\n\n#### **Uninterpretable + Unstable coefficients:**\n\n\n\n### **Intuition behind the instability:**\n\nIt might be a little difficult to digest exactly why the coefficients become highly unstable from the above explanation. To help give a visual aspect to the problem of collinearity, the following simulation was created to replicate the effects, and issues that arise with collinear features.\n\n```{r, results='hide', message=FALSE, warning = FALSE}\n# installing packages\n\nlibrary(tidyverse)\nlibrary(plotly)\n```\n\n\n```{r, collapse = FALSE, results=\"hold\"}\nset.seed(1)\n\nartificial_data <- data.frame(x1 = seq(1, 1000)) %>%\n    mutate(x2 = x1 + rnorm(1000, 0, 1), \n           y = (x1 + x2 + runif(1000, 0, 500))/4) \n\nartificial_data %>%\n    cor()\n```\n\n<iframe id=\"interactive\" src=\"https://erosrojas.shinyapps.io/lin_example/\" style=\"border: none; width: 1400px; height: 800px\" frameborder=\"0\"></iframe>"
  },
  {
    "objectID": "posts/multicollinearity/index.html#input-widget",
    "href": "posts/multicollinearity/index.html#input-widget",
    "title": "Multicollinearity: An Intuitive Example",
    "section": "Input widget",
    "text": "Input widget\n\n\nShow the code\nradioButtons(\n  inputId = \"radio_input\",\n  label = \"Are you a cat person or a dog person?\",\n  choices = c(\n    \"Cat person\",\n    \"Dog person\",\n    \"Don't make me choose!\",\n    \"Neither :(\"\n  ),\n  selected = \"Don't make me choose!\"\n)\n\n\n\nAre you a cat person or a dog person?\n\n\n\n\nCat person\n\n\n\n\n\nDog person\n\n\n\n\n\nDon't make me choose!\n\n\n\n\n\nNeither :("
  },
  {
    "objectID": "posts/multicollinearity/index.html#widget-value",
    "href": "posts/multicollinearity/index.html#widget-value",
    "title": "Multicollinearity: An Intuitive Example",
    "section": "Widget value",
    "text": "Widget value\n\n\n\n\n\nShow the code\nverbatimTextOutput(\"radio_output\")"
  }
]