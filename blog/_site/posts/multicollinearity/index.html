<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.654">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Eros Rojas">
<meta name="dcterms.date" content="2023-01-08">

<title>Home - Multicollinearity: An Intuitive Example</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Home</span>
  </a>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About Me</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/erosrojas"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://erosrojas.github.io/test.html"><i class="bi bi-card-list" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Multicollinearity: An Intuitive Example</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">R</div>
                <div class="quarto-category">Statistics</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Eros Rojas </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 8, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="an-intuitive-visualization-of-collinearity" class="level2">
<h2 class="anchored" data-anchor-id="an-intuitive-visualization-of-collinearity"><strong>An intuitive visualization of collinearity</strong></h2>
<p>Visualized in 3 dimensions, with the help of plotly. <a href="#intuition-behind-the-instability">(Skip to the interactive example)</a></p>
<p><br> <img src="3d.jpg" class="img-fluid"> <br> <br></p>
<section id="understanding-what-collinearity-is" class="level3">
<h3 class="anchored" data-anchor-id="understanding-what-collinearity-is"><strong>Understanding what collinearity is:</strong></h3>
<p>As defined by <a href="https://en.wikipedia.org/wiki/Multicollinearity" target="_blank">Wikipedia</a>, “<em>multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.</em>” Essentially, this means that an input variable (or multiple variables) is highly correlated with another input variable, to the extent that their relationship is virtually linearly dependent. This can cause numerous issues with respect to the regression analysis, most notably causing:</p>
<ul>
<li>
Inflated standard error
</li>
<li>
Uninterpretable + Unstable coefficients
</li>
</ul>
<p>All of these issues will be explained in the following 3 sections. <br> <br></p>
<section id="inflated-standard-error" class="level4">
<h4 class="anchored" data-anchor-id="inflated-standard-error"><strong>Inflated standard error:</strong></h4>
<p>The standard error of the coefficients of a linear model can be calculated as follows: <br><br> Lets assume a linear model has the following loss function</p>
<p><span class="math display">\[f(w) = ||Xw + \epsilon - \hat{y}||_2\]</span></p>
<p>where <span class="math inline">\(\epsilon\)</span> represents the error terms which follow a normal distribution, such that <span class="math inline">\(\epsilon \sim N(0, \sigma^2)\)</span>. Since the given loss function above is fully differentiable and convex, we can find a closed-form solution for its coefficients as</p>
<p><span class="math display">\[w = (X^TX)^{-1}X^T\hat{y}\]</span></p>
<p>Now that the coefficients are isolated, finding the variance should be quite straight forward. <span class="math display">\[\begin{align}
\mathrm{Var}(w) &amp;= \mathrm{Var}\left((X^TX)^{-1}X^T \hat{y} \right) \\
&amp;= (X^TX)^{−1}X^T \cdot \mathrm{Var}(\hat{y}) \cdot X(X^TX)^{−1} \\
&amp;= \sigma^2 (X^T X)^{-1}
\end{align}\]</span> <em>Note: <span class="math inline">\(\mathrm{Var}(\hat{y}) = \mathrm{Var}(\epsilon)\)</span> due to the fact that <span class="math inline">\(\hat{y} = Xw + \epsilon\)</span>, and <span class="math inline">\(Xw\)</span> has a variance of 0 (since <span class="math inline">\(Xw\)</span> is not a random variable)</em>. <br><br> It is clear that <span class="math inline">\(X^T X\)</span> must be a square matrix, therefore, it can be diagonalized such that</p>
<p><span class="math display">\[X^T X = PDP^{-1}\]</span></p>
<p>where <span class="math inline">\(P\)</span> is an invertible matrix containing linearly independent eigenvectors of <span class="math inline">\(X^T X\)</span>, and <span class="math inline">\(D\)</span> is a diagonal matrix containing the eigenvalues of <span class="math inline">\(X^T X\)</span> (you can read more about matrix diagonalization <a href="https://www.statlect.com/matrix-algebra/matrix-diagonalization" target="_blank">here</a>). Most notably, the product of the entries of <span class="math inline">\(D\)</span> (the eigenvalues) are equal to the determinant of <span class="math inline">\(D\)</span>. When multicollinearity is present, <span class="math inline">\(X^T X\)</span> becomes ‘nearly singular’ (a matrix which has a determinant very close to <span class="math inline">\(0\)</span>, and who’s columns are nearly linearly dependent. For reference, a non-singular matrix is linearly independent, and a singular matrix is linearly dependent and has a determinant of <span class="math inline">\(0\)</span>). In other words, this means that one or more of the eigenvalues of <span class="math inline">\(D\)</span> must be very close to <span class="math inline">\(0\)</span>, or in fact <span class="math inline">\(0\)</span>. Given the inverse of the diagonalized matrix above;</p>
<p><span class="math display">\[(X^T X)^{-1} = PD^{-1}P^{-1}\]</span></p>
<p>this means that with multicollinearity, <span class="math inline">\(D^{-1}\)</span> must have very large values. This is due to the fact that when the inverse of a matrix is taken, its determinant is essentially divided out from each element of the resulting matrix (this is an oversimplification, look into <a href="https://en.wikipedia.org/wiki/Adjugate_matrix" target="_blank">adjugate matrices</a> if you are interested). This causes <span class="math inline">\((X^T X)^{-1}\)</span> to subsequently also have very large values, which finally causes <span class="math inline">\(\mathrm{Var}(w)\)</span> to be very large. This large variance is what causes multicollinearity to severely inflate the standard error of the coefficients. <br> <br></p>
</section>
<section id="uninterpretable-unstable-coefficients" class="level4">
<h4 class="anchored" data-anchor-id="uninterpretable-unstable-coefficients"><strong>Uninterpretable + Unstable coefficients:</strong></h4>
<p>In addition to artificially inflating the standard error, collinearity is notorious for causing coefficients to become both uninterpretable, and unstable. To address the first concern, lets define a linear model with two (highly correlated) features:</p>
<p><span class="math display">\[ \hat{y} = b + x_1 w_1 + x_2 w_2 + \epsilon\]</span></p>
<p>In the case that <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are highly correlated, they can subsequently be described by one another, namely <span class="math inline">\(x_1 \approx x_2\)</span> (assuming <span class="math inline">\(w_1, w_2 &gt; 0\)</span>). If this is the case, then <span class="math display">\[\begin{align*}
    \hat{y} &amp;= b + x_1 w_1 + x_2 w_2 + \epsilon \\
    &amp;\approx b + x_1 w_1 + x_1 w_2 + \epsilon \\
    &amp;\approx b + x_1 (w_1 + w_2) + 0 \cdot x_2 + \epsilon
\end{align*}\]</span> If <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are perfectly correlated, then the weight of <span class="math inline">\(x_2\)</span> can be entirely transferred to <span class="math inline">\(x_1\)</span> (or vise versa) thus leaving <span class="math inline">\(x_2\)</span> with a coefficient of <span class="math inline">\(0\)</span>. This may or may not affect the output/predictions of the model, however it does affect how the coefficients are interpreted. For example, consider <span class="math inline">\(x_1\)</span> to be the square footage of a home, <span class="math inline">\(x_2\)</span> to be the number of rooms, and <span class="math inline">\(\hat{y}\)</span> to be the sell price of the given home. It is clear that <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> will be collinear (not perfectly collinear though since <span class="math inline">\(x_2\)</span> is discrete), therefore when calculating the coefficients of <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span>, getting a result near <span class="math inline">\(0\)</span> for <span class="math inline">\(w_2\)</span> would make no sense as increasing the number of rooms of a home should surely increase the sale price. <br> <br> Not only can it cause coefficients to zero out, it can also cause coefficients to drastically change. Consider the same initial equation as above: <span class="math display">\[\begin{align*}
    \hat{y} &amp;= b + x_1 w_1 + x_2 w_2 + \epsilon \\
    &amp;= b + x_1 w_1 + x_1 w_2 + \epsilon \\
    &amp;= b + x_1 (w_1 + w_2) + 0 \cdot x_2 + \epsilon \\
    &amp;= b + x_1 (w_1 + 10w_2) - 9w_2 x_2 + \epsilon
\end{align*}\]</span> Even though the coefficients are now out-of-wack with respect to the context of the problem (house prices), numerically they are equivalent to the initial linear model. <span class="math inline">\(x_2\)</span> now suggests that the number of rooms of a house has a negative relationship with the sale price of a house, which as we know is ridiculous. Likewise, there are an infinite number of combinations of <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> that satisfy the initial equation, since we now have an added degree of freedom in the feature matrix. That being said, the resulting values of <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> will depend on your data, and the severity of collinearity. This leads to the final main concern of multicollinearity: unstable coefficients.</p>
</section>
</section>
<section id="intuition-behind-the-instability" class="level3">
<h3 class="anchored" data-anchor-id="intuition-behind-the-instability"><strong>Intuition behind the instability:</strong></h3>
<p>If it was not clear enough above, the uninterpretability and instability of coefficients go hand-in-hand. As collinearity becomes perfect, the coefficients get more and more uninterpretable, and also get increasingly unstable. However, if you do not have a suitable background in linear algebra then it may still be unclear as to how exactly the instability affects the model output. To help give a visual aspect to the problem of collinearity, the following simulation was created to replicate the effects, and issues that arise with collinear features. <br> <br> Below is a 3D plot that represents the above linear model. <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are (nearly) perfect collinear features (some noise was included otherwise <span class="math inline">\(X\)</span> would be singular), and <span class="math inline">\(y\)</span> represents the output of the linear regression model which in this case is a regression plane since we have two input features. The visualization is meant to showcase how sensitive the coefficients are when collinear features are present. There are 1000 data points that make up the data, and by changing the range of the indices that create the linear model, it is clear how much the coefficients change (as seen by the drastic flips of the regression plane). Generally, taking a subset of your data should not change the underlying linear relationship. However, with collinear features even the slightest changes in data can shift the regression weights, as seen from the equations above. <br> <br> Try playing around with the range of the input to see how much the coefficients change.</p>
<div class="cell">

</div>
<iframe id="interactive" src="https://erosrojas.shinyapps.io/lin_example/" style="border: none; width: 1200px; height: 850px" frameborder="0">
</iframe>
<p>Hopefully this was able to shine some light as to what collinearity is, and the negative implications of collinear features within your data. 
<script type="application/shiny-prerendered" data-context="server-extras">
ojs_define <- function(..., .session = shiny::getDefaultReactiveDomain()) {
  quos <- rlang::enquos(...)
  vars <- rlang::list2(...)
  nm <- names(vars)
  if (is.null(nm)) {
    nm <- rep_len("", length(vars))
  }
  mapply(function(q, nm, val) {
    # Infer name, if possible
    if (nm == "") {
      tryCatch({
        nm <- rlang::as_name(q)
      }, error = function(e) {
        code <- paste(collapse = "\n", deparse(rlang::f_rhs(q)))
        stop("ojs_define() could not create a name for the argument: ", code)
      })
    }
    .session$output[[nm]] <- val
    outputOptions(.session$output, nm, suspendWhenHidden = FALSE)
    .session$sendCustomMessage("ojs-export", list(name = nm))
    NULL
  }, quos, nm, vars, SIMPLIFY = FALSE, USE.NAMES = FALSE)
  invisible()
}
</script>
</p>
<!--html_preserve-->
<script type="application/shiny-prerendered" data-context="dependencies">
{"type":"list","attributes":{},"value":[]}
</script>
<!--/html_preserve-->
<!--html_preserve-->

<script type="application/shiny-prerendered" data-context="execution_dependencies">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages","version"]},"class":{"type":"character","attributes":{},"value":["data.frame"]},"row.names":{"type":"integer","attributes":{},"value":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78]}},"value":[{"type":"character","attributes":{},"value":["assertthat","backports","base","broom","cellranger","cli","colorspace","compiler","crayon","datasets","DBI","dbplyr","digest","dplyr","ellipsis","evaluate","fansi","fastmap","forcats","fs","gargle","generics","ggplot2","glue","googledrive","googlesheets4","graphics","grDevices","grid","gtable","haven","hms","htmltools","htmlwidgets","httpuv","httr","jsonlite","knitr","later","lifecycle","lubridate","magrittr","methods","mime","modelr","munsell","pillar","pkgconfig","promises","purrr","R6","Rcpp","readr","readxl","reprex","rlang","rmarkdown","rvest","scales","shiny","stats","stringi","stringr","tibble","tidyr","tidyselect","tidyverse","timechange","tools","tzdb","utf8","utils","vctrs","withr","xfun","xml2","xtable","yaml"]},{"type":"character","attributes":{},"value":["0.2.1","1.4.1","4.2.2","1.0.1","1.1.0","3.4.1","2.0-3","4.2.2","1.5.2","4.2.2","1.1.3","2.2.1","0.6.30","1.0.10","0.3.2","0.18","1.0.3","1.1.0","0.5.2","1.5.2","1.2.1","0.1.3","3.4.0","1.6.2","2.0.0","1.0.1","4.2.2","4.2.2","4.2.2","0.3.1","2.5.1","1.1.2","0.5.4","1.5.4","1.6.6","1.4.4","1.8.3","1.41","1.3.0","1.0.3","1.9.0","2.0.3","4.2.2","0.12","0.1.10","0.5.0","1.8.1","2.0.3","1.2.0.1","0.3.5","2.5.1","1.0.9","2.1.3","1.4.1","2.0.2","1.0.6","2.20","1.0.3","1.2.1","1.7.4","4.2.2","1.7.8","1.4.1","3.1.8","1.2.1","1.2.0","1.3.2","0.1.1","4.2.2","0.3.0","1.2.2","4.2.2","0.5.1","2.5.0","0.36","1.3.3","1.8-4","2.3.6"]}]}]}
</script>
<!--/html_preserve-->


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Multicollinearity: An Intuitive Example"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Eros Rojas"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2023-01-08"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [R, Statistics]</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "colinear.jpg"</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: show</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">    code-summary: "Show the code"</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">    code-overflow: scroll</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">    css: styles.css</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="an">server:</span><span class="co"> shiny</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="fu">## **An intuitive visualization of collinearity**</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>Visualized in 3 dimensions, with the help of plotly. <span class="co">[</span><span class="ot">(Skip to the interactive example)</span><span class="co">](#intuition-behind-the-instability)</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;br&gt;</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="al">![](3d.jpg)</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;br&gt;</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;br&gt;</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="fu">### **Understanding what collinearity is:**</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>As defined by <span class="kw">&lt;a</span> <span class="er">href</span><span class="ot">=</span><span class="st">"https://en.wikipedia.org/wiki/Multicollinearity"</span> <span class="er">target</span><span class="ot">=</span><span class="st">"_blank"</span><span class="kw">&gt;</span>Wikipedia<span class="kw">&lt;/a&gt;</span>, "*multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.*" Essentially, this means that an input variable (or multiple variables) is highly correlated with another input variable, to the extent that their relationship is virtually linearly dependent. This can cause numerous issues with respect to the regression analysis, most notably causing: </span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;ul&gt;</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">&lt;li&gt;</span>Inflated standard error<span class="kw">&lt;/li&gt;</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">&lt;li&gt;</span>Uninterpretable + Unstable coefficients<span class="kw">&lt;/li&gt;</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/ul&gt;</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>All of these issues will be explained in the following 3 sections.</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;br&gt;</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;br&gt;</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="fu">#### **Inflated standard error:**</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>The standard error of the coefficients of a linear model can be calculated as follows: </span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;br&gt;&lt;br&gt;</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>Lets assume a linear model has the following loss function</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>$$f(w) = ||Xw + \epsilon - \hat{y}||_2$$</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>where $\epsilon$ represents the error terms which follow a normal distribution, such that $\epsilon \sim N(0, \sigma^2)$. Since the given loss function above is fully differentiable and convex, we can find a closed-form solution for its coefficients as</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>$$w = (X^TX)^{-1}X^T\hat{y}$$</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>Now that the coefficients are isolated, finding the variance should be quite straight forward.</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>\mathrm{Var}(w) &amp;= \mathrm{Var}\left((X^TX)^{-1}X^T \hat{y} \right) <span class="sc">\\</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>&amp;= (X^TX)^{−1}X^T \cdot \mathrm{Var}(\hat{y}) \cdot X(X^TX)^{−1} <span class="sc">\\</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>&amp;= \sigma^2 (X^T X)^{-1}</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>*Note: $\mathrm{Var}(\hat{y}) = \mathrm{Var}(\epsilon)$ due to the fact that $\hat{y} = Xw + \epsilon$, and $Xw$ has a variance of 0 (since $Xw$ is not a random variable)*.</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;br&gt;&lt;br&gt;</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>It is clear that $X^T X$ must be a square matrix, therefore, it can be diagonalized such that </span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>$$X^T X = PDP^{-1}$$</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>where $P$ is an invertible matrix containing linearly independent eigenvectors of $X^T X$, and $D$ is a diagonal matrix containing the eigenvalues of $X^T X$ (you can read more about matrix diagonalization <span class="kw">&lt;a</span> <span class="er">href</span><span class="ot">=</span><span class="st">"https://www.statlect.com/matrix-algebra/matrix-diagonalization"</span> <span class="er">target</span><span class="ot">=</span><span class="st">"_blank"</span><span class="kw">&gt;</span>here<span class="kw">&lt;/a&gt;</span>). Most notably, the product of the entries of $D$ (the eigenvalues) are equal to the determinant of $D$. When multicollinearity is present, $X^T X$ becomes 'nearly singular' (a matrix which has a determinant very close to $0$, and who's columns are nearly linearly dependent. For reference, a non-singular matrix is linearly independent, and a singular matrix is linearly dependent and has a determinant of $0$). In other words, this means that one or more of the eigenvalues of $D$ must be very close to $0$, or in fact $0$. Given the inverse of the diagonalized matrix above;</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>$$(X^T X)^{-1} = PD^{-1}P^{-1}$$</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>this means that with multicollinearity, $D^{-1}$ must have very large values. This is due to the fact that when the inverse of a matrix is taken, its determinant is essentially divided out from each element of the resulting matrix (this is an oversimplification, look into <span class="kw">&lt;a</span> <span class="er">href</span><span class="ot">=</span><span class="st">"https://en.wikipedia.org/wiki/Adjugate_matrix"</span> <span class="er">target</span><span class="ot">=</span><span class="st">"_blank"</span><span class="kw">&gt;</span>adjugate matrices<span class="kw">&lt;/a&gt;</span> if you are interested). This causes $(X^T X)^{-1}$ to subsequently also have very large values, which finally causes $\mathrm{Var}(w)$ to be very large. This large variance is what causes multicollinearity to severely inflate the standard error of the coefficients.</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;br&gt;</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;br&gt;</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="fu">#### **Uninterpretable + Unstable coefficients:**</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>In addition to artificially inflating the standard error, collinearity is notorious for causing coefficients to become both uninterpretable, and unstable. To address the first concern, lets define a linear model with two (highly correlated) features: </span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>$$ \hat{y} = b + x_1 w_1 + x_2 w_2 + \epsilon$$</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>In the case that $x_1$ and $x_2$ are highly correlated, they can subsequently be described by one another, namely $x_1 \approx x_2$ (assuming $w_1, w_2 &gt; 0$). If this is the case, then</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>\begin{align*} </span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>    \hat{y} &amp;= b + x_1 w_1 + x_2 w_2 + \epsilon <span class="sc">\\</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>    &amp;\approx b + x_1 w_1 + x_1 w_2 + \epsilon <span class="sc">\\</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>    &amp;\approx b + x_1 (w_1 + w_2) + 0 \cdot x_2 + \epsilon</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>If $x_1$ and $x_2$ are perfectly correlated, then the weight of $x_2$ can be entirely transferred to $x_1$ (or vise versa) thus leaving $x_2$ with a coefficient of $0$. This may or may not affect the output/predictions of the model, however it does affect how the coefficients are interpreted. For example, consider $x_1$ to be the square footage of a home, $x_2$ to be the number of rooms, and $\hat{y}$ to be the sell price of the given home. It is clear that $x_1$ and $x_2$ will be collinear (not perfectly collinear though since $x_2$ is discrete), therefore when calculating the coefficients of $w_1$ and $w_2$, getting a result near $0$ for $w_2$ would make no sense as increasing the number of rooms of a home should surely increase the sale price. </span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;br&gt;</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;br&gt;</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>Not only can it cause coefficients to zero out, it can also cause coefficients to drastically change. Consider the same initial equation as above:</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>\begin{align*} </span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>    \hat{y} &amp;= b + x_1 w_1 + x_2 w_2 + \epsilon <span class="sc">\\</span></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>    &amp;= b + x_1 w_1 + x_1 w_2 + \epsilon <span class="sc">\\</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>    &amp;= b + x_1 (w_1 + w_2) + 0 \cdot x_2 + \epsilon <span class="sc">\\</span></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>    &amp;= b + x_1 (w_1 + 10w_2) - 9w_2 x_2 + \epsilon</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>Even though the coefficients are now out-of-wack with respect to the context of the problem (house prices), numerically they are equivalent to the initial linear model. $x_2$ now suggests that the number of rooms of a house has a negative relationship with the sale price of a house, which as we know is ridiculous. Likewise, there are an infinite number of combinations of $w_1$ and $w_2$ that satisfy the initial equation, since we now have an added degree of freedom in the feature matrix. That being said, the resulting values of $w_1$ and $w_2$ will depend on your data, and the severity of collinearity. This leads to the final main concern of multicollinearity: unstable coefficients.</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="fu">### **Intuition behind the instability:**</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>If it was not clear enough above, the uninterpretability and instability of coefficients go hand-in-hand. As collinearity becomes perfect, the coefficients get more and more uninterpretable, and also get increasingly unstable. However, if you do not have a suitable background in linear algebra then it may still be unclear as to how exactly the instability affects the model output. To help give a visual aspect to the problem of collinearity, the following simulation was created to replicate the effects, and issues that arise with collinear features.</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;br&gt;</span></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;br&gt;</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>Below is a 3D plot that represents the above linear model. $x_1$ and $x_2$ are (nearly) perfect collinear features (some noise was included otherwise $X$ would be singular), and $y$ represents the output of the linear regression model which in this case is a regression plane since we have two input features. The visualization is meant to showcase how sensitive the coefficients are when collinear features are present. There are 1000 data points that make up the data, and by changing the range of the indices that create the linear model, it is clear how much the coefficients change (as seen by the drastic flips of the regression plane). Generally, taking a subset of your data should not change the underlying linear relationship. However, with collinear features even the slightest changes in data can shift the regression weights, as seen from the equations above. </span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;br&gt;</span></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;br&gt;</span></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>Try playing around with the range of the input to see how much the coefficients change.</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, collapse = FALSE, results="hold", warning = FALSE, message=FALSE, results='hide'}</span></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a><span class="co"># warning = FALSE</span></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a><span class="co"># results='hide'</span></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a><span class="co"># message=FALSE</span></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>artificial_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x1 =</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">1000</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">x2 =</span> x1 <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, <span class="dv">0</span>, <span class="dv">1</span>), </span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>           <span class="at">y =</span> (x1 <span class="sc">+</span> x2 <span class="sc">+</span> <span class="fu">runif</span>(<span class="dv">1000</span>, <span class="dv">0</span>, <span class="dv">500</span>))<span class="sc">/</span><span class="dv">4</span>) </span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>artificial_data <span class="sc">%&gt;%</span></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cor</span>()</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;iframe</span> <span class="er">id</span><span class="ot">=</span><span class="st">"interactive"</span> <span class="er">src</span><span class="ot">=</span><span class="st">"https://erosrojas.shinyapps.io/lin_example/"</span> <span class="er">style</span><span class="ot">=</span><span class="st">"border: none; width: 1200px; height: 850px"</span> <span class="er">frameborder</span><span class="ot">=</span><span class="st">"0"</span><span class="kw">&gt;&lt;/iframe&gt;</span></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>Hopefully this was able to shine some light as to what collinearity is, and the negative implications of collinear features within your data.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>