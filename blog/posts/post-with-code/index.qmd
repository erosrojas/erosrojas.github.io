---
title: "My First Data Science Project"
author: "Eros Rojas"
date: "2022-07-30"
categories: [R, Projects]
image: "ancient.jpg"

format:
  html:
    code-fold: show
    code-summary: "Show the code"
    code-tools: true
    code-overflow: scroll
---

## **Breaking down my first data science project**
TLDR: Do not assume anything of your data. Often times you will be wrong.

<br>
![](data-pic.jpg)
<br>

Back when I was in my first year of university, I was very new to the data science and statistical modeling world. I was vastly unaware of the number of specialized algorithms created for basically any, and all types of applications. 

This was one of my first full data science projects, and as a result of my limited knowledge, there are numerous aspects of the report that are misleading or flat out incorrect. I wish to make this post as a point of progression to see how much I have learned in the past two years, and to be able to recognize what types of mistakes data science beginners generally make.

Below you will find my first data science project, but with the added bonus of italicized and coloured annotations alongside all of the questionable and dubious assumptions that were made. 

<br>
<br>
<br>

## **Predicting COVID-19 Transmission Risk**

##### **Introduction:**

We plan to help out Billy, who has recently lost his job due to COVID-19 and is trying to find a job in another country. He prefers to move to a country that will be safer, with fewer new COVID cases. 

We aim to help Billy answer the question: “How safe is it to move to country X?” by predicting the number of new_cases_per_million through KNN-Regression. A lower number of cases indicates a safer country with lower transmission risk. We will use a categorical variable “risk level”, that we created ourselves, to more easily interpret the numerical output of our prediction. 

We will use worldwide data collected over the past few months of the pandemic ([Ritchie 2020](https://ourworldindata.org/coronavirus-source-data)). This dataset contains many variables, but we will use population density, stringency index (a composite measure of government COVID-19 response), GDP per capita (economic output per person), hospital beds per thousand, and life expectancy as predictors. We narrowed the data down to consider only current data (from September 1st 2020 onwards), to accurately model the current coronavirus situation. We will predict the number of new cases per million, using our designated predictor variables, to inform Billy’s decision.


```{r, results='hide', message=FALSE, warning = FALSE}
# installing 3rd party packages required for running this file
install.packages("lubridate", repos = "http://cran.us.r-project.org")
install.packages("GGally", repos = "http://cran.us.r-project.org")
install.packages("reshape2", repos = "http://cran.us.r-project.org")
#install.packages("shiny", repos = "http://cran.us.r-project.org")
install.packages("repr", repos = "http://cran.us.r-project.org")

# loading installed packages
library(tidyverse)
library(tidymodels)
library(repr)
library(lubridate)
library(GGally)
library(reshape2)
#library(shiny)
```

The data has been obtained from the ['Our World in Data,' Coronavirus Source Dataset](https://ourworldindata.org/coronavirus-source-data), which is collected in partnership with the University of Oxford and the Oxford Martin School, updated daily.

```{r, collapse = FALSE, results="hold"}
# utilizng the github commit hash code in order to maintain the same running code throughout the lifespan of this project
url <- "https://raw.githubusercontent.com/owid/covid-19-data/fb73a3759b6691dc9a6f880353a37e70cd7ceb92/public/data/owid-covid-data.csv"
data <- read_csv(url, show_col_types = FALSE)

# adding table title
#strong("Table 1: Raw data from 'Coronavirus Source Data'")
head(data)
```

We begin by selecting the columns which will be useful for exploratory analysis. We will be visualizing the 'new_cases_smoothed_per_million' to represent a smoothed average (prettier/smoother plots!). However, for our predictive model, we will be using 'new_cases_per_million' for a more specific estimate of new cases for one given day.

We restricted the observations to those taken past September 1st 2020 – to ensure recent data and prevent overplotting – and before November 16th 2020, since Billy was booking his flight on November 17th. Therefore, the model will be unable to ‘cheat’ by seeing the data that Billy used to make his prediction/decision.

```{r, collapse = FALSE, results="hold"}
# this is our exploratory data table, depicting our predictors of interest
newdata <- data %>%
  select(location, date, total_cases, new_cases, new_cases_per_million ,new_cases_smoothed_per_million, stringency_index, population_density, gdp_per_capita, hospital_beds_per_thousand, life_expectancy) %>% 
  filter(date >= as.Date("2020-09-01"), date <= as.Date("2020-11-16")) %>% #filter date after september to avoid overplotting
  filter(new_cases_smoothed_per_million >= 0 & new_cases_smoothed_per_million < 1000) %>% #stating our upper and lower boundries
  drop_na() # we are dropping our NA values due to the fact that we can not preform KNN regression on values which do not exist 

# adding table title
#strong("Table 2: Preliminary data filtering")
head(newdata)
```

*<span style="color:#b30000">incomplete, post has been placed on the backburner. will be back another day to finish. feel free to check out my newest post <a href="https://erosrojas.github.io/blog/_site/posts/multicollinearity/">here</a></span>*
