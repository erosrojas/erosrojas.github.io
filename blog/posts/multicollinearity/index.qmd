---
title: "Multicollinearity: An Intuitive Example"
author: "Eros Rojas"
date: "2023-01-08"
categories: [R, Statistics]
image: "colinear.jpg"

format:
  html:
    code-fold: show
    code-summary: "Show the code"
    code-tools: true
    code-overflow: scroll
---

## **An intuitive visualization of collinearity**
Visualized in 3 dimensions, with the help of plotly.

<br>
![](3d.jpg)
<br>
<br>

### **Understanding what collinearity is:**

As defined by [Wikipedia](https://en.wikipedia.org/wiki/Multicollinearity), "*multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.*" Essentially, this means that an input variable (or multiple variables) is highly correlated with another input variable, to the extent that their relationship is virtually linearly dependent. This can cause numerous issues with respect to the regression analysis including: 

<ul>
    <li>Inflated standard error</li>
    <li>Non-unique solutions for the coefficients</li>
    <li>Uninterpretable/Unstable coefficients</li>
</ul>

All of these issues will be explained in the following 3 sections.
<br>
<br>

#### **Inflated standard error:**

The standard error of the coefficients of a linear model can be calculated as follows: 
<br>
<br>
Lets assume 
$$f(w) = ||Xw + \epsilon - \hat{y}||_2$$
where $\epsilon$ represents the error terms which follow a normal distribution, such that $\epsilon \sim N(0, \sigma^2)$. Since the standard linear model above has a fully differentiable and convex loss function, we can find a closed-form solution for its coefficients as
$$w = (X^TX)^{-1}X^T\hat{y}$$
Now that the coefficients are isolated, finding the variance should be quite straight forward.
\begin{align}
\mathrm{Var}(w) &= \mathrm{Var}\left((X^TX)^{-1}X^T \hat{y} \right) \\
&= (X^TX)^{−1}X^T \cdot \mathrm{Var}(\hat{y}) \cdot X(X^TX)^{−1} \\
&= \sigma^2 (X^T X)^{-1}
\end{align}
*Note: $\mathrm{Var}(\hat{y}) = \mathrm{Var}(\epsilon)$ due to the fact that $\hat{y} = Xw + \epsilon$ and $X$ and $w$ inherently have a variance of 0*.


