---
title: "Multicollinearity: An Intuitive Example"
author: "Eros Rojas"
date: "2023-01-08"
categories: [R, Statistics]
image: "colinear.jpg"

format:
  html:
    code-fold: show
    code-summary: "Show the code"
    code-tools: true
    code-overflow: scroll
---

## **An intuitive visualization of collinearity**
Visualized in 3 dimensions, with the help of plotly.

<br>
![](3d.jpg)
<br>
<br>

### **Understanding what collinearity is:**

As defined by <a href="https://en.wikipedia.org/wiki/Multicollinearity" target="_blank">Wikipedia</a>, "*multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.*" Essentially, this means that an input variable (or multiple variables) is highly correlated with another input variable, to the extent that their relationship is virtually linearly dependent. This can cause numerous issues with respect to the regression analysis including: 

<ul>
    <li>Inflated standard error</li>
    <li>Non-unique solutions for the coefficients</li>
    <li>Uninterpretable/Unstable coefficients</li>
</ul>

All of these issues will be explained in the following 3 sections.
<br>
<br>

#### **Inflated standard error:**

The standard error of the coefficients of a linear model can be calculated as follows: 
<br><br>
Lets assume a linear model has the following loss function
$$f(w) = ||Xw + \epsilon - \hat{y}||_2$$
where $\epsilon$ represents the error terms which follow a normal distribution, such that $\epsilon \sim N(0, \sigma^2)$. Since the given loss function above is fully differentiable and convex, we can find a closed-form solution for its coefficients as
$$w = (X^TX)^{-1}X^T\hat{y}$$
Now that the coefficients are isolated, finding the variance should be quite straight forward.
\begin{align}
\mathrm{Var}(w) &= \mathrm{Var}\left((X^TX)^{-1}X^T \hat{y} \right) \\
&= (X^TX)^{−1}X^T \cdot \mathrm{Var}(\hat{y}) \cdot X(X^TX)^{−1} \\
&= \sigma^2 (X^T X)^{-1}
\end{align}
*Note: $\mathrm{Var}(\hat{y}) = \mathrm{Var}(\epsilon)$ due to the fact that $\hat{y} = Xw + \epsilon$, and $Xw$ has a variance of 0 (since $Xw$ is not a random variable)*.
<br><br>
It is clear that $X^T X$ must be a square matrix, therefore, it can be diagonalized such that 
$$X^T X = PDP^{-1}$$
where $P$ is an invertible matrix containing linearly independent eigenvectors of $X^T X$, and $D$ is a diagonal matrix containing the eigenvalues of $X^T X$ (you can read more about matrix diagonalization <a href="https://www.statlect.com/matrix-algebra/matrix-diagonalization" target="_blank">here</a>). Most notably, the product of the entries of $D$ (the eigenvalues) are equal to the determinant of $D$. When multicollinearity is present, $X^T X$ becomes 'nearly singular' (a matrix which has a determinant very close to $0$, and who's columns are nearly linearly dependent. For reference, a non-singular matrix is linearly independent, and a singular matrix is linearly dependent and has a determinant of $0$). In other words, this means that one or more of the eigenvalues of $D$ must be very close to $0$, or in fact $0$. Given the inverse of the diagonalized matrix above;
$$(X^T X)^{-1} = PD^{-1}P^{-1}$$
this means that with multicollinearity, $D^{-1}$ must have very large values. This is due to the fact that when the inverse of a matrix is taken, its determinant is essentially divided out from each element of the resulting matrix (this is an oversimplification, look into <a href="https://en.wikipedia.org/wiki/Adjugate_matrix" target="_blank">adjugate matrices</a> if you are interested). This causes $(X^T X)^{-1}$ to subsequently also have very large values, which finally causes $\mathrm{Var}(w)$ to be very large. This shows why multicollinearity severely inflates the standard error of the coefficients.
<br>
<br>

#### **Non-unique solutions:**

Next

