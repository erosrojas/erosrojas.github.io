{
  "hash": "fc702cd59599d9f5a67317acc4d55e19",
  "result": {
    "markdown": "---\ntitle: \"My Old DSCI-100 Project\"\nauthor: \"Eros Rojas\"\ndate: \"2022-07-30\"\ncategories: [R, DSCI-100]\nimage: \"ancient.jpg\"\n\nformat:\n  html:\n    code-fold: show\n    code-summary: \"Show the code\"\n    code-tools: true\n    code-overflow: scroll\n---\n\n\n## **Breaking down my old DSCI-100 project**\nTLDR: Do not assume anything of your data. Often times you will be wrong.\n\n<br>\n![](data-pic.jpg)\n<br>\n\nBack when I took DSCI-100, I was very new to the data science and statistical modeling world. I was vastly unaware of the absurd number of specialized algorithms created for basically any and all types of problems. \n\nThis was one of my first full data science reports, and as a result of my previously limited knowledge, there are numerous aspects of the report that are misleading, or flat out misrepresentative of reality. I wish to make this post as a point of progression to see how much I have learned in the past two years, and to be able to recognize what types of mistakes data science beginners generally make. Think of it like a video game 'remaster', except the difference being that nobody asked for it.\n\nBelow you will find my DSCI-100 final report, but with the added bonus of italicized annotations alongside all of the questionable statements and pieces of code that were written. \n\n<br>\n<br>\n<br>\n\n## **Predicting COVID-19 Transmission Risk**\n\n##### **Introduction:**\n\nWe plan to help out Billy, who has recently lost his job due to COVID-19 and is trying to find a job in another country. He prefers to move to a country that will be safer, with fewer new COVID cases. \n\nWe aim to help Billy answer the question: “How safe is it to move to country X?” by predicting the number of new_cases_per_million through KNN-Regression. A lower number of cases indicates a safer country with lower transmission risk. We will use a categorical variable “risk level”, that we created ourselves, to more easily interpret the numerical output of our prediction. \n\nWe will use worldwide data collected over the past few months of the pandemic ([Ritchie 2020](https://ourworldindata.org/coronavirus-source-data)). This dataset contains many variables, but we will use population density, stringency index (a composite measure of government COVID-19 response), GDP per capita (economic output per person), hospital beds per thousand, and life expectancy as predictors. We narrowed the data down to consider only current data (from September 1st 2020 onwards), to accurately model the current coronavirus situation. We will predict the number of new cases per million, using our designated predictor variables, to inform Billy’s decision.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# installing 3rd party packages required for running this file\ninstall.packages(\"lubridate\", repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"GGally\", repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"reshape2\", repos = \"http://cran.us.r-project.org\")\n#install.packages(\"shiny\", repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"repr\", repos = \"http://cran.us.r-project.org\")\n\n# loading installed packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(repr)\nlibrary(lubridate)\nlibrary(GGally)\nlibrary(reshape2)\n#library(shiny)\n```\n:::\n\n\nThe data has been obtained from the ['Our World in Data,' Coronavirus Source Dataset](https://ourworldindata.org/coronavirus-source-data), which is collected in partnership with the University of Oxford and the Oxford Martin School, updated daily.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# utilizng the github commit hash code in order to maintain the same running code throughout the lifespan of this project\nurl <- \"https://raw.githubusercontent.com/owid/covid-19-data/fb73a3759b6691dc9a6f880353a37e70cd7ceb92/public/data/owid-covid-data.csv\"\ndata <- read_csv(url, show_col_types = FALSE)\n\n# adding table title\n#strong(\"Table 1: Raw data from 'Coronavirus Source Data'\")\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 50\n  iso_code continent location date       total…¹ new_c…² new_c…³ total…⁴ new_d…⁵\n  <chr>    <chr>     <chr>    <date>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 AFG      Asia      Afghani… 2019-12-31      NA       0      NA      NA       0\n2 AFG      Asia      Afghani… 2020-01-01      NA       0      NA      NA       0\n3 AFG      Asia      Afghani… 2020-01-02      NA       0      NA      NA       0\n4 AFG      Asia      Afghani… 2020-01-03      NA       0      NA      NA       0\n5 AFG      Asia      Afghani… 2020-01-04      NA       0      NA      NA       0\n6 AFG      Asia      Afghani… 2020-01-05      NA       0      NA      NA       0\n# … with 41 more variables: new_deaths_smoothed <dbl>,\n#   total_cases_per_million <dbl>, new_cases_per_million <dbl>,\n#   new_cases_smoothed_per_million <dbl>, total_deaths_per_million <dbl>,\n#   new_deaths_per_million <dbl>, new_deaths_smoothed_per_million <dbl>,\n#   reproduction_rate <dbl>, icu_patients <dbl>,\n#   icu_patients_per_million <dbl>, hosp_patients <dbl>,\n#   hosp_patients_per_million <dbl>, weekly_icu_admissions <dbl>, …\n```\n:::\n:::\n\n\nWe begin by selecting the columns which will be useful for exploratory analysis. We will be visualizing the 'new_cases_smoothed_per_million' to represent a smoothed average (prettier/smoother plots!). However, for our predictive model, we will be using 'new_cases_per_million' for a more specific estimate of new cases for one given day.\n\nWe restricted the observations to those taken past September 1st 2020 – to ensure recent data and prevent overplotting – and before November 16th 2020, since Billy was booking his flight on November 17th. Therefore, the model will be unable to ‘cheat’ by seeing the data that Billy used to make his prediction/decision.\n\n*<span style=\"color:#b30000\">incomplete, will be back another day to finish</span>*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# this is our exploratory data table, depicting our predictors of interest\nnewdata <- data %>%\n  select(location, date, total_cases, new_cases, new_cases_per_million ,new_cases_smoothed_per_million, stringency_index, population_density, gdp_per_capita, hospital_beds_per_thousand, life_expectancy) %>% \n  filter(date >= as.Date(\"2020-09-01\"), date <= as.Date(\"2020-11-16\")) %>% #filter date after september to avoid overplotting\n  filter(new_cases_smoothed_per_million >= 0 & new_cases_smoothed_per_million < 1000) %>% #stating our upper and lower boundries\n  drop_na() # we are dropping our NA values due to the fact that we can not preform KNN regression on values which do not exist \n\n# adding table title\n#strong(\"Table 2: Preliminary data filtering\")\nhead(newdata)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 11\n  location    date       total…¹ new_c…² new_c…³ new_c…⁴ strin…⁵ popul…⁶ gdp_p…⁷\n  <chr>       <date>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 Afghanistan 2020-09-01   38196      34   0.873   0.462    21.3    54.4   1804.\n2 Afghanistan 2020-09-02   38205       9   0.231   0.492    21.3    54.4   1804.\n3 Afghanistan 2020-09-03   38243      38   0.976   0.429    21.3    54.4   1804.\n4 Afghanistan 2020-09-04   38288      45   1.16    0.583    21.3    54.4   1804.\n5 Afghanistan 2020-09-05   38304      16   0.411   0.602    21.3    54.4   1804.\n6 Afghanistan 2020-09-06   38324      20   0.514   0.664    21.3    54.4   1804.\n# … with 2 more variables: hospital_beds_per_thousand <dbl>,\n#   life_expectancy <dbl>, and abbreviated variable names ¹​total_cases,\n#   ²​new_cases, ³​new_cases_per_million, ⁴​new_cases_smoothed_per_million,\n#   ⁵​stringency_index, ⁶​population_density, ⁷​gdp_per_capita\n```\n:::\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}