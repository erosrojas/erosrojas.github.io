{
  "hash": "776bc0e9ccee3adea81e926ecfc68c98",
  "result": {
    "markdown": "---\ntitle: \"Multicollinearity: An Intuitive Example\"\nauthor: \"Eros Rojas\"\ndate: \"2023-01-08\"\ncategories: [R, Statistics]\nimage: \"colinear.jpg\"\nformat:\n  html:\n    code-fold: show\n    code-summary: \"Show the code\"\n    code-tools: true\n    code-overflow: scroll\n    css: styles.css\nserver: shiny\n---\n\n\n\n\n## **An intuitive visualization of collinearity**\nVisualized in 3 dimensions, with the help of plotly.\n\n<br>\n![](3d.jpg)\n<br>\n<br>\n\n### **Understanding what collinearity is:**\n\nAs defined by <a href=\"https://en.wikipedia.org/wiki/Multicollinearity\" target=\"_blank\">Wikipedia</a>, \"*multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.*\" Essentially, this means that an input variable (or multiple variables) is highly correlated with another input variable, to the extent that their relationship is virtually linearly dependent. This can cause numerous issues with respect to the regression analysis, most notably causing: \n\n<ul>\n    <li>Inflated standard error</li>\n    <li>Uninterpretable + Unstable coefficients</li>\n</ul>\n\nAll of these issues will be explained in the following 3 sections.\n<br>\n<br>\n\n#### **Inflated standard error:**\n\nThe standard error of the coefficients of a linear model can be calculated as follows: \n<br><br>\nLets assume a linear model has the following loss function\n\n$$f(w) = ||Xw + \\epsilon - \\hat{y}||_2$$\n\nwhere $\\epsilon$ represents the error terms which follow a normal distribution, such that $\\epsilon \\sim N(0, \\sigma^2)$. Since the given loss function above is fully differentiable and convex, we can find a closed-form solution for its coefficients as\n\n$$w = (X^TX)^{-1}X^T\\hat{y}$$\n\nNow that the coefficients are isolated, finding the variance should be quite straight forward.\n\\begin{align}\n\\mathrm{Var}(w) &= \\mathrm{Var}\\left((X^TX)^{-1}X^T \\hat{y} \\right) \\\\\n&= (X^TX)^{−1}X^T \\cdot \\mathrm{Var}(\\hat{y}) \\cdot X(X^TX)^{−1} \\\\\n&= \\sigma^2 (X^T X)^{-1}\n\\end{align}\n*Note: $\\mathrm{Var}(\\hat{y}) = \\mathrm{Var}(\\epsilon)$ due to the fact that $\\hat{y} = Xw + \\epsilon$, and $Xw$ has a variance of 0 (since $Xw$ is not a random variable)*.\n<br><br>\nIt is clear that $X^T X$ must be a square matrix, therefore, it can be diagonalized such that \n\n$$X^T X = PDP^{-1}$$\n\nwhere $P$ is an invertible matrix containing linearly independent eigenvectors of $X^T X$, and $D$ is a diagonal matrix containing the eigenvalues of $X^T X$ (you can read more about matrix diagonalization <a href=\"https://www.statlect.com/matrix-algebra/matrix-diagonalization\" target=\"_blank\">here</a>). Most notably, the product of the entries of $D$ (the eigenvalues) are equal to the determinant of $D$. When multicollinearity is present, $X^T X$ becomes 'nearly singular' (a matrix which has a determinant very close to $0$, and who's columns are nearly linearly dependent. For reference, a non-singular matrix is linearly independent, and a singular matrix is linearly dependent and has a determinant of $0$). In other words, this means that one or more of the eigenvalues of $D$ must be very close to $0$, or in fact $0$. Given the inverse of the diagonalized matrix above;\n\n$$(X^T X)^{-1} = PD^{-1}P^{-1}$$\n\nthis means that with multicollinearity, $D^{-1}$ must have very large values. This is due to the fact that when the inverse of a matrix is taken, its determinant is essentially divided out from each element of the resulting matrix (this is an oversimplification, look into <a href=\"https://en.wikipedia.org/wiki/Adjugate_matrix\" target=\"_blank\">adjugate matrices</a> if you are interested). This causes $(X^T X)^{-1}$ to subsequently also have very large values, which finally causes $\\mathrm{Var}(w)$ to be very large. This shows why multicollinearity severely inflates the standard error of the coefficients.\n<br>\n<br>\n\n#### **Uninterpretable + Unstable coefficients:**\n\n\n\n### **Intuition behind the instability:**\n\nIt might be a little difficult to digest exactly why the coefficients become highly unstable from the above explanation. To help give a visual aspect to the problem of collinearity, the following simulation was created to replicate the effects, and issues that arise with collinear features.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n          x1        x2         y\nx1 1.0000000 0.9999936 0.9699297\nx2 0.9999936 1.0000000 0.9699606\ny  0.9699297 0.9699606 1.0000000\n```\n:::\n:::\n\n\n\n\n \n<iframe id=\"interactive\" src=\"https://erosrojas.shinyapps.io/lin_example/\" style=\"border: none; width: 1200px; height: 900px\" frameborder=\"0\"></iframe>\n\n\nContinued\npreserve46e58da3a7b00598\n\n<!--html_preserve-->\n<script type=\"application/shiny-prerendered\" data-context=\"dependencies\">\n{\"type\":\"list\",\"attributes\":{},\"value\":[]}\n</script>\n<!--/html_preserve-->\n<!--html_preserve-->\n<script type=\"application/shiny-prerendered\" data-context=\"execution_dependencies\">\n{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"packages\"]}},\"value\":[{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"packages\",\"version\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"data.frame\"]},\"row.names\":{\"type\":\"integer\",\"attributes\":{},\"value\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"assertthat\",\"backports\",\"base\",\"broom\",\"cellranger\",\"cli\",\"colorspace\",\"compiler\",\"crayon\",\"data.table\",\"datasets\",\"DBI\",\"dbplyr\",\"digest\",\"dplyr\",\"ellipsis\",\"evaluate\",\"fansi\",\"fastmap\",\"forcats\",\"fs\",\"gargle\",\"generics\",\"ggplot2\",\"glue\",\"googledrive\",\"googlesheets4\",\"graphics\",\"grDevices\",\"grid\",\"gtable\",\"haven\",\"hms\",\"htmltools\",\"htmlwidgets\",\"httpuv\",\"httr\",\"jsonlite\",\"knitr\",\"later\",\"lazyeval\",\"lifecycle\",\"lubridate\",\"magrittr\",\"methods\",\"mime\",\"modelr\",\"munsell\",\"pillar\",\"pkgconfig\",\"plotly\",\"promises\",\"purrr\",\"R6\",\"Rcpp\",\"readr\",\"readxl\",\"reprex\",\"rlang\",\"rmarkdown\",\"rvest\",\"scales\",\"shiny\",\"stats\",\"stringi\",\"stringr\",\"tibble\",\"tidyr\",\"tidyselect\",\"tidyverse\",\"timechange\",\"tools\",\"tzdb\",\"utf8\",\"utils\",\"vctrs\",\"viridisLite\",\"withr\",\"xfun\",\"xml2\",\"xtable\",\"yaml\"]},{\"type\":\"character\",\"attributes\":{},\"value\":[\"0.2.1\",\"1.4.1\",\"4.2.2\",\"1.0.1\",\"1.1.0\",\"3.4.1\",\"2.0-3\",\"4.2.2\",\"1.5.2\",\"1.14.6\",\"4.2.2\",\"1.1.3\",\"2.2.1\",\"0.6.30\",\"1.0.10\",\"0.3.2\",\"0.18\",\"1.0.3\",\"1.1.0\",\"0.5.2\",\"1.5.2\",\"1.2.1\",\"0.1.3\",\"3.4.0\",\"1.6.2\",\"2.0.0\",\"1.0.1\",\"4.2.2\",\"4.2.2\",\"4.2.2\",\"0.3.1\",\"2.5.1\",\"1.1.2\",\"0.5.4\",\"1.5.4\",\"1.6.6\",\"1.4.4\",\"1.8.3\",\"1.41\",\"1.3.0\",\"0.2.2\",\"1.0.3\",\"1.9.0\",\"2.0.3\",\"4.2.2\",\"0.12\",\"0.1.10\",\"0.5.0\",\"1.8.1\",\"2.0.3\",\"4.10.1\",\"1.2.0.1\",\"0.3.5\",\"2.5.1\",\"1.0.9\",\"2.1.3\",\"1.4.1\",\"2.0.2\",\"1.0.6\",\"2.20\",\"1.0.3\",\"1.2.1\",\"1.7.4\",\"4.2.2\",\"1.7.8\",\"1.4.1\",\"3.1.8\",\"1.2.1\",\"1.2.0\",\"1.3.2\",\"0.1.1\",\"4.2.2\",\"0.3.0\",\"1.2.2\",\"4.2.2\",\"0.5.1\",\"0.4.1\",\"2.5.0\",\"0.36\",\"1.3.3\",\"1.8-4\",\"2.3.6\"]}]}]}\n</script>\n<!--/html_preserve-->",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {
      "preserve46e58da3a7b00598": "\n<script type=\"application/shiny-prerendered\" data-context=\"server-extras\">\nojs_define <- function(..., .session = shiny::getDefaultReactiveDomain()) {\n  quos <- rlang::enquos(...)\n  vars <- rlang::list2(...)\n  nm <- names(vars)\n  if (is.null(nm)) {\n    nm <- rep_len(\"\", length(vars))\n  }\n  mapply(function(q, nm, val) {\n    # Infer name, if possible\n    if (nm == \"\") {\n      tryCatch({\n        nm <- rlang::as_name(q)\n      }, error = function(e) {\n        code <- paste(collapse = \"\\n\", deparse(rlang::f_rhs(q)))\n        stop(\"ojs_define() could not create a name for the argument: \", code)\n      })\n    }\n    .session$output[[nm]] <- val\n    outputOptions(.session$output, nm, suspendWhenHidden = FALSE)\n    .session$sendCustomMessage(\"ojs-export\", list(name = nm))\n    NULL\n  }, quos, nm, vars, SIMPLIFY = FALSE, USE.NAMES = FALSE)\n  invisible()\n}\n</script>\n"
    },
    "postProcess": true
  }
}